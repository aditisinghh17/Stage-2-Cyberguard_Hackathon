{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train=pd.read_csv(\"C:/Users/hp/Downloads/embedding_data_after_preproocessing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>crimeaditionalinfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Online and Social Media Related Crime</td>\n",
       "      <td>Cyber Bullying  Stalking  Sexting</td>\n",
       "      <td>continue received random call abusive message ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Fraud CallVishing</td>\n",
       "      <td>fraudster continuously messaging asking pay mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Online Gambling  Betting</td>\n",
       "      <td>Online Gambling  Betting</td>\n",
       "      <td>acting like police demanding money adding sect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online and Social Media Related Crime</td>\n",
       "      <td>Online Job Fraud</td>\n",
       "      <td>job applied job interview telecalling resource...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Fraud CallVishing</td>\n",
       "      <td>received call lady stating send new phone vivo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                category                       sub_category  \\\n",
       "0  Online and Social Media Related Crime  Cyber Bullying  Stalking  Sexting   \n",
       "1                 Online Financial Fraud                  Fraud CallVishing   \n",
       "2               Online Gambling  Betting           Online Gambling  Betting   \n",
       "3  Online and Social Media Related Crime                   Online Job Fraud   \n",
       "4                 Online Financial Fraud                  Fraud CallVishing   \n",
       "\n",
       "                                  crimeaditionalinfo  \n",
       "0  continue received random call abusive message ...  \n",
       "1  fraudster continuously messaging asking pay mo...  \n",
       "2  acting like police demanding money adding sect...  \n",
       "3  job applied job interview telecalling resource...  \n",
       "4  received call lady stating send new phone vivo...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FastTextModel:\n",
    "    def __init__(self, vector_size=100, window=5, min_count=5, neg_samples=5, \n",
    "                 learning_rate=0.05, min_n=3, max_n=6):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.neg_samples = neg_samples\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_n = min_n\n",
    "        self.max_n = max_n\n",
    "        \n",
    "        self.word_vectors = {}\n",
    "        self.context_vectors = {}\n",
    "        self.ngram_vectors = {}\n",
    "        self.word_frequencies = {}\n",
    "        self.vocab = set()\n",
    "        self.total_words = 0\n",
    "        \n",
    "    def generate_ngrams(self, word):\n",
    "        \"\"\"Generate character n-grams for a word\"\"\"\n",
    "        ngrams = []\n",
    "        word = f\"<{word}>\"  # Add boundary markers\n",
    "        for n in range(self.min_n, min(self.max_n + 1, len(word) + 1)):\n",
    "            for i in range(len(word) - n + 1):\n",
    "                ngrams.append(word[i:i+n])\n",
    "        return ngrams\n",
    "    \n",
    "    def initialize_vector(self):\n",
    "        \"\"\"Initialize a new vector with small random values\"\"\"\n",
    "        return np.random.uniform(-0.5/self.vector_size, 0.5/self.vector_size, \n",
    "                               (self.vector_size,))\n",
    "    \n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"Build vocabulary and initialize vectors\"\"\"\n",
    "        # Count word frequencies\n",
    "        print(\"Building vocabulary...\")\n",
    "        for sentence in tqdm(sentences):\n",
    "            for word in sentence:\n",
    "                self.word_frequencies[word] = self.word_frequencies.get(word, 0) + 1\n",
    "                self.total_words += 1\n",
    "        \n",
    "        # Filter by minimum count and create vocabulary\n",
    "        self.vocab = {word for word, freq in self.word_frequencies.items() \n",
    "                     if freq >= self.min_count}\n",
    "        \n",
    "        # Initialize word and context vectors\n",
    "        print(\"Initializing vectors...\")\n",
    "        for word in tqdm(self.vocab):\n",
    "            self.word_vectors[word] = self.initialize_vector()\n",
    "            self.context_vectors[word] = self.initialize_vector()\n",
    "            \n",
    "            # Initialize n-gram vectors\n",
    "            for ngram in self.generate_ngrams(word):\n",
    "                if ngram not in self.ngram_vectors:\n",
    "                    self.ngram_vectors[ngram] = self.initialize_vector()\n",
    "    \n",
    "    def get_word_vector(self, word):\n",
    "        \"\"\"Get vector representation of a word (including subword information)\"\"\"\n",
    "        if word in self.word_vectors:\n",
    "            vector = self.word_vectors[word].copy()\n",
    "        else:\n",
    "            vector = np.zeros(self.vector_size)\n",
    "            \n",
    "        # Add subword information\n",
    "        ngrams = self.generate_ngrams(word)\n",
    "        ngram_count = 0\n",
    "        for ngram in ngrams:\n",
    "            if ngram in self.ngram_vectors:\n",
    "                vector += self.ngram_vectors[ngram]\n",
    "                ngram_count += 1\n",
    "        \n",
    "        if ngram_count > 0:\n",
    "            vector /= (ngram_count + 1)  # +1 for word vector if exists\n",
    "            \n",
    "        return vector\n",
    "    \n",
    "    def negative_sampling(self, n):\n",
    "        \"\"\"Sample negative examples based on word frequency\"\"\"\n",
    "        neg_samples = []\n",
    "        word_list = list(self.vocab)\n",
    "        frequencies = np.array([self.word_frequencies[word] for word in word_list])\n",
    "        probs = frequencies ** 0.75\n",
    "        probs = probs / probs.sum()\n",
    "        \n",
    "        while len(neg_samples) < n:\n",
    "            sample = np.random.choice(word_list, p=probs)\n",
    "            if sample not in neg_samples:\n",
    "                neg_samples.append(sample)\n",
    "                \n",
    "        return neg_samples\n",
    "    \n",
    "    def train_pair(self, target, context, negative=True):\n",
    "        \"\"\"Train on a single target-context pair\"\"\"\n",
    "        # Get target vector\n",
    "        target_vector = self.get_word_vector(target)\n",
    "        \n",
    "        if negative:\n",
    "            # Negative sampling\n",
    "            contexts = [context] + self.negative_sampling(self.neg_samples)\n",
    "            labels = [1] + [0] * self.neg_samples\n",
    "        else:\n",
    "            contexts = [context]\n",
    "            labels = [1]\n",
    "            \n",
    "        loss = 0\n",
    "        # Update for each context\n",
    "        for ctx, label in zip(contexts, labels):\n",
    "            context_vector = self.context_vectors[ctx]\n",
    "            \n",
    "            # Forward pass\n",
    "            score = np.dot(target_vector, context_vector)\n",
    "            prob = 1 / (1 + np.exp(-score))  # sigmoid\n",
    "            \n",
    "            # Compute error\n",
    "            error = label - prob\n",
    "            loss += -label * np.log(prob) - (1 - label) * np.log(1 - prob)\n",
    "            \n",
    "            # Compute gradients\n",
    "            grad = error * self.learning_rate\n",
    "            \n",
    "            # Update vectors\n",
    "            if target in self.word_vectors:\n",
    "                self.word_vectors[target] += grad * context_vector\n",
    "            \n",
    "            # Update n-gram vectors\n",
    "            for ngram in self.generate_ngrams(target):\n",
    "                if ngram in self.ngram_vectors:\n",
    "                    self.ngram_vectors[ngram] += grad * context_vector\n",
    "            \n",
    "            self.context_vectors[ctx] += grad * target_vector\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def train_epoch(self, sentences):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        total_loss = 0\n",
    "        total_pairs = 0\n",
    "        \n",
    "        for sentence in tqdm(sentences):\n",
    "            for i, target in enumerate(sentence):\n",
    "                # Define context window\n",
    "                start = max(0, i - self.window)\n",
    "                end = min(len(sentence), i + self.window + 1)\n",
    "                \n",
    "                # Train on context words\n",
    "                for j in range(start, end):\n",
    "                    if i != j and sentence[j] in self.vocab:\n",
    "                        loss = self.train_pair(target, sentence[j])\n",
    "                        total_loss += loss\n",
    "                        total_pairs += 1\n",
    "                        \n",
    "        return total_loss / total_pairs if total_pairs > 0 else float('inf')\n",
    "    \n",
    "    def train(self, sentences, epochs=8):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            epoch_loss = self.train_epoch(sentences)\n",
    "            losses.append(epoch_loss)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Loss: {epoch_loss:.4f}\")\n",
    "            print(f\"Time: {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def save_vectors(self, filename):\n",
    "        \"\"\"Save word vectors to file\"\"\"\n",
    "        print(f\"Saving vectors to {filename}\")\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            # Write header\n",
    "            f.write(f\"{len(self.vocab)} {self.vector_size}\\n\")\n",
    "            \n",
    "            # Write word vectors\n",
    "            for word in self.vocab:\n",
    "                vector = self.get_word_vector(word)\n",
    "                vector_str = ' '.join(str(x) for x in vector)\n",
    "                f.write(f\"{word} {vector_str}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing\"\"\"\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77539/77539 [00:00<00:00, 83161.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16556/16556 [00:00<00:00, 20827.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 967/77539 [7:43:35<615:18:25, 28.93s/it]   "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load your dataset\n",
    "    df = df_train.copy()\n",
    "    \n",
    "    # Preprocess text\n",
    "    sentences = [preprocess_text(text) for text in df['crimeaditionalinfo'].values if isinstance(text, str)]\n",
    "    \n",
    "    # Initialize model\n",
    "    model = FastTextModel(\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=5,\n",
    "        neg_samples=5,\n",
    "        learning_rate=0.05,\n",
    "        min_n=3,\n",
    "        max_n=6\n",
    "    )\n",
    "    \n",
    "    # Build vocabulary\n",
    "    model.build_vocab(sentences)\n",
    "    \n",
    "    # Train model\n",
    "    losses = model.train(sentences, epochs=3)\n",
    "    \n",
    "    # Save vectors\n",
    "    model.save_vectors('fasttext_vectors.txt')\n",
    "    \n",
    "    # Plot training loss\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig('training_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
