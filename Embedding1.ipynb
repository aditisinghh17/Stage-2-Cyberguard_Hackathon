{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md2OHapPAfS7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "\n",
        "class FastTextEmbeddings:\n",
        "    def __init__(self, min_count: int = 5, ngram_range: Tuple[int, int] = (3, 6),\n",
        "                 embedding_dim: int = 100, window_size: int = 5):\n",
        "        self.min_count = min_count\n",
        "        self.ngram_range = ngram_range\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.window_size = window_size\n",
        "        self.word2idx = {}\n",
        "        self.ngram2idx = {}\n",
        "        self.model = None\n",
        "\n",
        "    def generate_ngrams(self, word: str) -> List[str]:\n",
        "        \"\"\"Generate character n-grams for a word\"\"\"\n",
        "        word = f\"<{word}>\"\n",
        "        ngrams = []\n",
        "        for n in range(self.ngram_range[0], self.ngram_range[1] + 1):\n",
        "            for i in range(len(word) - n + 1):\n",
        "                ngrams.append(word[i:i + n])\n",
        "        return ngrams\n",
        "\n",
        "    def build_vocab(self, texts: List[str]):\n",
        "        \"\"\"Build vocabulary from texts\"\"\"\n",
        "        # Count words and build word vocabulary\n",
        "        word_counts = Counter()\n",
        "        for text in texts:\n",
        "            words = self._preprocess_text(text)\n",
        "            word_counts.update(words)\n",
        "\n",
        "        for word, count in word_counts.items():\n",
        "            if count >= self.min_count:\n",
        "                self.word2idx[word] = len(self.word2idx)\n",
        "\n",
        "        # Build ngram vocabulary\n",
        "        ngram_counts = Counter()\n",
        "        for word in self.word2idx.keys():\n",
        "            ngrams = self.generate_ngrams(word)\n",
        "            ngram_counts.update(ngrams)\n",
        "\n",
        "        for ngram in ngram_counts:\n",
        "            self.ngram2idx[ngram] = len(self.ngram2idx)\n",
        "\n",
        "    def _preprocess_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Clean and tokenize text\"\"\"\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        return text.split()\n",
        "\n",
        "class EmbeddingDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], fasttext: FastTextEmbeddings):\n",
        "        self.texts = texts\n",
        "        self.fasttext = fasttext\n",
        "        self.pairs = self._create_training_pairs()\n",
        "\n",
        "    def _create_training_pairs(self):\n",
        "        pairs = []\n",
        "        for text in self.texts:\n",
        "            words = self.fasttext._preprocess_text(text)\n",
        "            for i, word in enumerate(words):\n",
        "                if word not in self.fasttext.word2idx:\n",
        "                    continue\n",
        "\n",
        "                # Get context words within window\n",
        "                for j in range(max(0, i - self.fasttext.window_size),\n",
        "                             min(len(words), i + self.fasttext.window_size + 1)):\n",
        "                    if i != j and words[j] in self.fasttext.word2idx:\n",
        "                        pairs.append((self.fasttext.word2idx[word],\n",
        "                                    self.fasttext.word2idx[words[j]]))\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.pairs[idx][0]), torch.tensor(self.pairs[idx][1])\n",
        "\n",
        "class FastTextModel(nn.Module):\n",
        "    def __init__(self, vocab_size: int, ngram_vocab_size: int, embedding_dim: int):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.ngram_embeddings = nn.Embedding(ngram_vocab_size, embedding_dim)\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, word_idx, context_idx):\n",
        "        word_embed = self.word_embeddings(word_idx)\n",
        "        context_embed = self.context_embeddings(context_idx)\n",
        "        return torch.mul(word_embed, context_embed).sum(dim=1)\n",
        "\n",
        "    def get_word_embedding(self, word: str, fasttext: FastTextEmbeddings) -> torch.Tensor:\n",
        "        \"\"\"Get embedding for a word including its ngrams\"\"\"\n",
        "        if word not in fasttext.word2idx:\n",
        "            # For unknown words, use only ngram embeddings\n",
        "            ngrams = fasttext.generate_ngrams(word)\n",
        "            ngram_vectors = []\n",
        "            for ngram in ngrams:\n",
        "                if ngram in fasttext.ngram2idx:\n",
        "                    idx = fasttext.ngram2idx[ngram]\n",
        "                    ngram_vectors.append(self.ngram_embeddings(torch.tensor([idx])))\n",
        "            if not ngram_vectors:\n",
        "                return torch.zeros(self.word_embeddings.embedding_dim)\n",
        "            return torch.mean(torch.stack(ngram_vectors), dim=0)\n",
        "\n",
        "        # For known words, combine word and ngram embeddings\n",
        "        word_idx = fasttext.word2idx[word]\n",
        "        word_vector = self.word_embeddings(torch.tensor([word_idx]))\n",
        "\n",
        "        ngrams = fasttext.generate_ngrams(word)\n",
        "        ngram_vectors = []\n",
        "        for ngram in ngrams:\n",
        "            if ngram in fasttext.ngram2idx:\n",
        "                idx = fasttext.ngram2idx[ngram]\n",
        "                ngram_vectors.append(self.ngram_embeddings(torch.tensor([idx])))\n",
        "\n",
        "        if ngram_vectors:\n",
        "            ngram_vector = torch.mean(torch.stack(ngram_vectors), dim=0)\n",
        "            return word_vector + ngram_vector\n",
        "        return word_vector\n",
        "\n",
        "def train_fasttext(texts: List[str], embedding_dim: int = 100, epochs: int = 5):\n",
        "    # Initialize FastText\n",
        "    fasttext = FastTextEmbeddings(embedding_dim=embedding_dim)\n",
        "    fasttext.build_vocab(texts)\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = EmbeddingDataset(texts, fasttext)\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = FastTextModel(\n",
        "        vocab_size=len(fasttext.word2idx),\n",
        "        ngram_vocab_size=len(fasttext.ngram2idx),\n",
        "        embedding_dim=embedding_dim\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for word_idx, context_idx in dataloader:\n",
        "            word_idx = word_idx.to(device)\n",
        "            context_idx = context_idx.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(word_idx, context_idx)\n",
        "            loss = criterion(output, torch.ones_like(output))\n",
        "\n",
        "            # Add negative sampling\n",
        "            neg_context_idx = torch.randint(0, len(fasttext.word2idx),\n",
        "                                          context_idx.shape).to(device)\n",
        "            neg_output = model(word_idx, neg_context_idx)\n",
        "            loss += criterion(neg_output, torch.zeros_like(neg_output))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}')\n",
        "\n",
        "    fasttext.model = model\n",
        "    return fasttext\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample texts (replace with your cybercrime descriptions)\n",
        "    texts = [\n",
        "        \"cyber fraud online payment\",\n",
        "        \"phishing email bank account\",\n",
        "        # Add more texts...\n",
        "    ]\n",
        "\n",
        "    # Train model\n",
        "    fasttext = train_fasttext(texts, embedding_dim=100, epochs=5)\n",
        "\n",
        "    # Get embeddings for words\n",
        "    word = \"cyber\"\n",
        "    if word in fasttext.word2idx:\n",
        "        embedding = fasttext.model.get_word_embedding(word, fasttext)\n",
        "        print(f\"Embedding for '{word}':\", embedding.detach().numpy())"
      ]
    }
  ]
}