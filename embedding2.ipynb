{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbGKLZ_p_Uzy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "from typing import List, Dict, Set, Tuple\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "class UnigramTokenizer:\n",
        "    def __init__(self, vocab_size: int = 32000, min_freq: int = 3):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.min_freq = min_freq\n",
        "        self.subword_vocab = {}\n",
        "        self.char_vocab = set()\n",
        "        self.piece2idx = {}\n",
        "        self.idx2piece = {}\n",
        "\n",
        "    def train(self, texts: List[str]):\n",
        "        \"\"\"Train tokenizer on the corpus\"\"\"\n",
        "        print(\"Training tokenizer...\")\n",
        "        # Count initial subwords (characters and common sequences)\n",
        "        char_freq = Counter()\n",
        "        subword_freq = Counter()\n",
        "\n",
        "        # First pass: count characters and build initial subwords\n",
        "        for text in tqdm(texts):\n",
        "            chars = list(text.lower())\n",
        "            char_freq.update(chars)\n",
        "\n",
        "            # Build initial subwords (2-6 character sequences)\n",
        "            for i in range(len(chars)):\n",
        "                for length in range(2, 7):\n",
        "                    if i + length <= len(chars):\n",
        "                        subword = ''.join(chars[i:i + length])\n",
        "                        subword_freq[subword] += 1\n",
        "\n",
        "        # Filter by minimum frequency\n",
        "        subword_freq = Counter({k: v for k, v in subword_freq.items()\n",
        "                              if v >= self.min_freq})\n",
        "\n",
        "        # Build final vocabulary\n",
        "        vocab = set()\n",
        "        # Add all characters first\n",
        "        vocab.update(char_freq.keys())\n",
        "\n",
        "        # Add most frequent subwords until vocab_size is reached\n",
        "        remaining_size = self.vocab_size - len(vocab)\n",
        "        for subword, _ in subword_freq.most_common(remaining_size):\n",
        "            vocab.add(subword)\n",
        "\n",
        "        # Create piece to index mappings\n",
        "        self.piece2idx = {piece: idx for idx, piece in enumerate(sorted(vocab))}\n",
        "        self.idx2piece = {idx: piece for piece, idx in self.piece2idx.items()}\n",
        "        print(f\"Vocabulary size: {len(self.piece2idx)}\")\n",
        "\n",
        "    def tokenize(self, text: str) -> List[int]:\n",
        "        \"\"\"Tokenize text into subword indices\"\"\"\n",
        "        text = text.lower()\n",
        "        tokens = []\n",
        "        while len(text) > 0:\n",
        "            max_match = None\n",
        "            max_length = 0\n",
        "\n",
        "            # Find longest matching subword\n",
        "            for length in range(min(6, len(text)), 0, -1):\n",
        "                subword = text[:length]\n",
        "                if subword in self.piece2idx:\n",
        "                    max_match = subword\n",
        "                    max_length = length\n",
        "                    break\n",
        "\n",
        "            if max_match is None:\n",
        "                # If no match found, take single character\n",
        "                tokens.append(self.piece2idx.get(text[0], self.piece2idx['<unk>']))\n",
        "                text = text[1:]\n",
        "            else:\n",
        "                tokens.append(self.piece2idx[max_match])\n",
        "                text = text[max_length:]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "class EmbeddingDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], tokenizer: UnigramTokenizer,\n",
        "                 context_size: int = 5):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.context_size = context_size\n",
        "        self.examples = []\n",
        "\n",
        "        print(\"Creating training examples...\")\n",
        "        for text in tqdm(texts):\n",
        "            tokens = self.tokenizer.tokenize(text)\n",
        "\n",
        "            # Create context windows\n",
        "            for i in range(len(tokens)):\n",
        "                # Get context indices\n",
        "                context_start = max(0, i - context_size)\n",
        "                context_end = min(len(tokens), i + context_size + 1)\n",
        "                context = tokens[context_start:i] + tokens[i+1:context_end]\n",
        "\n",
        "                # Add positive example\n",
        "                self.examples.append((tokens[i], context))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        target, context = self.examples[idx]\n",
        "        # Pad context if needed\n",
        "        context_padded = context + [0] * (2 * self.context_size - len(context))\n",
        "        return torch.tensor(target), torch.tensor(context_padded)\n",
        "\n",
        "class MultilingualEmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int = 300):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Main embedding layers\n",
        "        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Character-level CNN\n",
        "        self.char_cnn = nn.Sequential(\n",
        "            nn.Conv1d(embedding_dim, embedding_dim, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(embedding_dim, embedding_dim, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Initialize embeddings\n",
        "        self.target_embeddings.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.context_embeddings.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def forward(self, target_idx, context_idxs):\n",
        "        # Get target embedding\n",
        "        target_embed = self.target_embeddings(target_idx)\n",
        "\n",
        "        # Get context embeddings and apply char CNN\n",
        "        context_embeds = self.context_embeddings(context_idxs)\n",
        "        context_embeds = self.char_cnn(context_embeds.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        # Average context embeddings\n",
        "        context_embed = context_embeds.mean(dim=1)\n",
        "\n",
        "        # Compute similarity\n",
        "        similarity = torch.sum(target_embed * context_embed, dim=1)\n",
        "        return similarity\n",
        "\n",
        "    def get_embedding(self, token_ids: List[int]) -> torch.Tensor:\n",
        "        \"\"\"Get embedding for a sequence of tokens\"\"\"\n",
        "        with torch.no_grad():\n",
        "            token_tensor = torch.tensor(token_ids)\n",
        "            embeddings = self.target_embeddings(token_tensor)\n",
        "            embeddings = self.char_cnn(embeddings.unsqueeze(0).transpose(1, 2))\n",
        "            embeddings = embeddings.transpose(1, 2).squeeze(0)\n",
        "            return embeddings.mean(dim=0)\n",
        "\n",
        "def train_model(texts: List[str], embedding_dim: int = 300, epochs: int = 5,\n",
        "                batch_size: int = 64, vocab_size: int = 32000):\n",
        "    # Initialize tokenizer and train on corpus\n",
        "    tokenizer = UnigramTokenizer(vocab_size=vocab_size)\n",
        "    tokenizer.train(texts)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = EmbeddingDataset(texts, tokenizer)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = MultilingualEmbeddingModel(vocab_size=len(tokenizer.piece2idx),\n",
        "                                     embedding_dim=embedding_dim)\n",
        "\n",
        "    # Training setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    print(f\"Training on {device}...\")\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(dataloader)\n",
        "        for batch_idx, (target, context) in enumerate(progress_bar):\n",
        "            target = target.to(device)\n",
        "            context = context.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Positive samples\n",
        "            pos_similarity = model(target, context)\n",
        "            pos_loss = criterion(pos_similarity, torch.ones_like(pos_similarity))\n",
        "\n",
        "            # Negative sampling\n",
        "            neg_context = torch.randint(0, len(tokenizer.piece2idx),\n",
        "                                      context.shape).to(device)\n",
        "            neg_similarity = model(target, neg_context)\n",
        "            neg_loss = criterion(neg_similarity, torch.zeros_like(neg_similarity))\n",
        "\n",
        "            # Combined loss\n",
        "            loss = pos_loss + neg_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_description(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def get_text_embedding(text: str, model: MultilingualEmbeddingModel,\n",
        "                      tokenizer: UnigramTokenizer) -> torch.Tensor:\n",
        "    \"\"\"Get embedding for a complete text\"\"\"\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return model.get_embedding(tokens)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample texts (replace with your actual data)\n",
        "    texts = [\n",
        "        \"maine online loan liya aur payment nahi kar paya\",\n",
        "        \"nenu loan teesukuni repayment cheyyalekapoyanu\",\n",
        "        \"naan loan eduthu thiruppi katta mudiyala\",\n",
        "        # Add your 2.2M texts here\n",
        "    ]\n",
        "\n",
        "    # Train model\n",
        "    model, tokenizer = train_model(texts, embedding_dim=300, epochs=5)\n",
        "\n",
        "    # Get embedding for new text\n",
        "    test_text = \"maine payment nahi kiya\"\n",
        "    embedding = get_text_embedding(test_text, model, tokenizer)\n",
        "    print(f\"Embedding shape: {embedding.shape}\")"
      ]
    }
  ]
}