# -*- coding: utf-8 -*-
"""Retagging after Pre-processing using Bi-Directional Encoder with LSTM .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hvOhGNU33RV2S3QrSQ4BAheB5IrlJQDV
"""

import pandas as pd
df1=pd.read_csv("/content/train.csv")
df2=pd.read_csv("/content/test.csv")

# Combine using concat()
df_train = pd.concat([df1, df2])

df_train.head()

print("Dataset Shape:", df_train.shape)
print("\nColumns in the dataset:", df_train.columns.tolist())

"""### **Empty or whitespace values, missing values, duplicates, charecters length < 150**"""

df_train_cleaned = df_train.copy()
df_train_cleaned.dropna(subset=['crimeaditionalinfo'] ,inplace=True)

# 1. Empty or whitespace values
def strip_warn(row):
    try:
        x = row.strip()
        return x
    except:
        print(row)
        return ""


print(df_train_cleaned[df_train_cleaned['crimeaditionalinfo'].str.strip().str.len() == 0].shape)
df_train_cleaned['crimeaditionalinfo'] = df_train_cleaned['crimeaditionalinfo'].apply(strip_warn)
df_train_cleaned = df_train_cleaned[df_train_cleaned['crimeaditionalinfo'].str.strip().str.len() > 0]

# 2. Missing values
print(df_train_cleaned.isnull().sum())

# df_train_cleaned=df_train_cleaned.dropna(subset=["crimeaditionalinfo"])

# 3. Duplicates
print(df_train_cleaned.duplicated().sum())

df_train_cleaned=df_train_cleaned.drop_duplicates()

print(df_train_cleaned['crimeaditionalinfo'].duplicated().sum())

df_train_cleaned = df_train_cleaned.drop_duplicates(subset=['crimeaditionalinfo'])

# 4. Character length check
count_less_than_150 = df_train_cleaned[df_train_cleaned['crimeaditionalinfo'].str.len() <= 150].shape[0]
print("Number of rows with less than 150 characters in the crimeaditionalinfo column:", count_less_than_150)

# Keep only rows where text length is > 150 characters
df_train_cleaned = df_train_cleaned[df_train_cleaned['crimeaditionalinfo'].str.len() > 150]

# Reset index
df_train_cleaned.reset_index(drop=True, inplace=True)

df_train_cleaned.shape

pd.set_option('display.max_colwidth', None)
pd.set_option('display.max_rows', None)

df_train_cleaned.sample(10)

"""### **Reduce noise in data**"""

! pip install contractions -q

# Import required libraries
import re
import unicodedata
import contractions


# Fix encoding and handle accented characters
df_train_cleaned['crimeaditionalinfo'] = df_train_cleaned['crimeaditionalinfo'].apply(lambda x: unicodedata.normalize('NFKD', x))
df_train_cleaned['crimeaditionalinfo'] = df_train_cleaned['crimeaditionalinfo'].apply(lambda x: x.encode('ascii', 'ignore').decode('utf-8'))

# Expand contractions
df_train_cleaned['crimeaditionalinfo'] = df_train_cleaned['crimeaditionalinfo'].apply(lambda x: contractions.fix(x))

# Remove noise but keep periods
df_train_cleaned['crimeaditionalinfo'] = df_train_cleaned['crimeaditionalinfo'].apply(lambda x: x.lower())
df_train_cleaned['crimeaditionalinfo'] = df_train_cleaned['crimeaditionalinfo'].apply(lambda x: re.sub(r'\S+@\S+', '', x))
df_train_cleaned['crimeaditionalinfo'] = df_train_cleaned['crimeaditionalinfo'].apply(lambda x: re.sub(r'http\S+|www.\S+', '', x))
# Modified to keep periods while removing other punctuation
df_train_cleaned['crimeaditionalinfo'] = df_train_cleaned['crimeaditionalinfo'].apply(lambda x: re.sub(r'[^\w\s\.]', '', x))
df_train_cleaned['crimeaditionalinfo'] = df_train_cleaned['crimeaditionalinfo'].apply(lambda x: re.sub(r'\s+', ' ', x))
df_train_cleaned['crimeaditionalinfo'] = df_train_cleaned['crimeaditionalinfo'].apply(lambda x: x.strip())

df_train_cleaned.sample(10)

"""Language detection for stop word removal
(on hold)
"""

'''!pip install indic-nlp-library langdetect
!pip install polyglot
!pip install pyicu
!pip install pycld2

import pandas as pd
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from indicnlp.tokenize import indic_tokenize
from indicnlp.normalize import indic_normalize
import re
import nltk

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

def preprocess_text(text):
    if not isinstance(text, str):
        return ""

    # Normalize Indic text
    if lang in ["hi", "bn", "ta", "te", "mr"]:
        normalizer = indic_normalize.Normalizer()
        text = normalizer.normalize(text)

    # Remove special characters and digits
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)

    # Tokenization
    words = text.split() if lang == "en" else indic_tokenize.trivial_tokenize(text)

    # Stopword removal and lemmatization
    if lang == "en":
        words = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stop_words]
    elif lang in ["hi", "bn", "ta", "te", "mr"]:
        words = [word for word in words if word not in indic_stopwords]

    return " ".join(words)

'''

"""## **Stop-word removal, lemmatization, vectorization, tokenization, normalization**"""

! pip install nltk indic-nlp-library cleanlab unidecode autocorrect -q

import re
import nltk
import unidecode
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')

stop_words = set(stopwords.words('english'))
indic_stopwords = set([
    "aur", "ki", "hai", "huyi", "ho", "mein", "ye", "ke", "jo", "saath", "ko",
    "bhi", "tatha", "par", "se", "kisi", "un", "apna", "tum", "main", "aap", "inhe",
    "in", "abhi", "ab", "woh", "hum", "unka", "is", "us", "kintu", "athva", "nahin",
    "kar","firto","fir","kese","esse","ka","kabhi","karna"
])
stop_words=stop_words.union(indic_stopwords)

lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Lowercasing
    text = text.lower()

    # Remove special characters and punctuation
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

    # Remove extra whitespaces
    text = re.sub(r'\s+', ' ', text).strip()

    # Normalize Unicode characters
    text = unidecode.unidecode(text)

    # Tokenization
    words = word_tokenize(text)

    # Lemmatization
    words = [lemmatizer.lemmatize(word) for word in words]

    # Removing stopwords
    words = [word for word in words if word not in stop_words]

    return ' '.join(words)

# Apply preprocessing
df_train_cleaned["crimeaditionalinfo"] = df_train_cleaned.apply(lambda row: preprocess_text(row["crimeaditionalinfo"]), axis=1)

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import cleanlab
from cleanlab.classification import CleanLearning
from sklearn.linear_model import LogisticRegression

# Vectorize text using TF-IDF
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df_train_cleaned["crimeaditionalinfo"])

'''
#Dont run unless you really have to check
tf_idf = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names_out())

final_df = tf_idf

print("{} rows".format(final_df.shape[0]))
final_df.T.nlargest(5, 0)
'''

"""## Checking Mislabelled Data

AutoEncoder with LSTM Approach
"""

!pip install tensorflow -q

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential,Model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Input, RepeatVector, TimeDistributed
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest

df=df_train_cleaned.copy()

# Load Pretrained GloVe Embeddings (100D)
embedding_index = {}
with open("/content/glove.6B.100d.txt", encoding="utf-8") as f:  # Replace with actual GloVe file
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype="float32")
        embedding_index[word] = coefs

# Tokenization & Padding
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(df['crimeaditionalinfo'])
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(df['crimeaditionalinfo'])
padded_sequences = pad_sequences(sequences, maxlen=200, padding='post')

# Prepare Embedding Matrix
embedding_dim = 100  # Must match GloVe vector dimension
embedding_matrix = np.zeros((5000, embedding_dim))
for word, i in word_index.items():
    if i < 5000:
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

# Encode labels
label_encoder = LabelEncoder()
df['category_encoded'] = label_encoder.fit_transform(df['category'])
num_classes = len(label_encoder.classes_)

# One-hot encode labels for multi-class classification
y_categorical = to_categorical(df['category_encoded'], num_classes=num_classes)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, y_categorical, test_size=0.2, random_state=42)

import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import Callback
from sklearn.metrics import accuracy_score

class AccuracyCallback(Callback):
    def __init__(self, X_train, threshold_percentile=95):
        super(AccuracyCallback, self).__init__()
        self.X_train = X_train
        self.threshold_percentile = threshold_percentile

    def on_epoch_end(self, epoch, logs=None):
        X_train_pred = self.model.predict(self.X_train, verbose=0)

        # Compute Mean Squared Error per sample
        mse_per_sample = np.mean(np.power(self.X_train - X_train_pred, 2), axis=1)

        # Determine threshold for misclassification (top 5% MSE)
        threshold = np.percentile(mse_per_sample, self.threshold_percentile)

        # Binary classification: 1 = Correct, 0 = Mislabeled
        y_pred = (mse_per_sample < threshold).astype(int)  # Correct = 1, Mislabeled = 0
        y_true = np.ones_like(y_pred)  # All training samples are considered correct

        # Compute Accuracy
        accuracy = accuracy_score(y_true, y_pred)

        print(f"Epoch {epoch+1}: Accuracy = {accuracy:.4f}")

# Attach the callback during training
accuracy_callback = AccuracyCallback(X_train)

# Define parameters
latent_dim = 128  # Latent space size
max_seq_length = 200  # Adjust according to your data
vocab_size = 5000  # Define based on your dataset

# Encoder
input_layer = Input(shape=(max_seq_length,))
embedding_layer = Embedding(input_dim=vocab_size, output_dim=100, trainable=True)(input_layer)
encoder = Bidirectional(LSTM(latent_dim, activation="tanh", return_sequences=False, dropout=0.2, recurrent_dropout=0.2))(embedding_layer)

# Decoder
decoder = RepeatVector(max_seq_length)(encoder)  # Repeat latent vector for each timestep
decoder = LSTM(256, activation="tanh", return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(decoder)
decoder = TimeDistributed(Dense(num_classes, activation="softmax"))(decoder)  # Softmax for multiclass

# Model
autoencoder = Model(input_layer, decoder)

from tensorflow.keras.optimizers import Adam
optimizer = Adam(learning_rate=0.0001, clipnorm=1.0)
autoencoder.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
# Summary
autoencoder.summary()

# Train the Autoencoder
history = autoencoder.fit(X_train, np.repeat(y_train[:, np.newaxis, :], 200, axis=1),
                          epochs=5, validation_data=(X_test, np.repeat(y_test[:, np.newaxis, :], 200, axis=1)),
                          batch_size=32)

# Extract encoded representations
encoder_model = Model(input_layer, encoder)
encoded_features = encoder_model.predict(X_test)  # Extract latent vectors for anomaly detection

# Train Isolation Forest (Unsupervised Anomaly Detector)
iso_forest = IsolationForest(contamination=0.05, random_state=42)
iso_forest.fit(encoded_features)

test_indices = df.index[X_test.shape[0]:]
df_test = df.iloc[test_indices].copy()
df_test['anomaly_flag'] = iso_forest.predict(encoded_features)
df_test['anomaly_flag'] = df_test['anomaly_flag'].map({1: 0, -1: 1})

# Ground truth mislabeled cases
y_pred = autoencoder.predict(X_test)
y_pred_labels = np.argmax(y_pred[:, 0, :], axis=1)
df_test['predicted_category'] = label_encoder.inverse_transform(y_pred_labels)
df_test['mislabel_flag'] = df_test['category'] != df_test['predicted_category']

# Save DataFrame as CSV
df_test.to_csv('relabelled_data_encoder.csv', index=False)

# Compute Performance Metrics
accuracy = accuracy_score(df_test['mislabel_flag'], df_test['anomaly_flag'])
precision = precision_score(df_test['mislabel_flag'], df_test['anomaly_flag'])
recall = recall_score(df_test['mislabel_flag'], df_test['anomaly_flag'])
f1 = f1_score(df_test['mislabel_flag'], df_test['anomaly_flag'])

# Print Results
print(f"Number of mislabeled cases detected: {df_test['anomaly_flag'].sum()}")
print(f"Anomaly Detection Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")